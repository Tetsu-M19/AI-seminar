{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10404,
     "status": "ok",
     "timestamp": 1662958423417,
     "user": {
      "displayName": "山口哲弘",
      "userId": "06897743095690117498"
     },
     "user_tz": -540
    },
    "id": "l7GrhvdwaUO9",
    "outputId": "efc5bb38-5d04-47ff-c9d5-cd03f51e3cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: segmentation_models_pytorch in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: tqdm in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from segmentation_models_pytorch) (4.47.0)\n",
      "Requirement already satisfied: pillow in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from segmentation_models_pytorch) (7.2.0)\n",
      "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from segmentation_models_pytorch) (0.7.1)\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from segmentation_models_pytorch) (0.11.2)\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from segmentation_models_pytorch) (0.7.4)\n",
      "Requirement already satisfied: timm==0.4.12 in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from segmentation_models_pytorch) (0.4.12)\n",
      "Requirement already satisfied: torch in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.10.1)\n",
      "Requirement already satisfied: numpy in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.18.5)\n",
      "Requirement already satisfied: munch in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.7.4.2)\n",
      "Requirement already satisfied: six in /home/sozolab/anaconda3/envs/py36/lib/python3.6/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34936,
     "status": "ok",
     "timestamp": 1660717033814,
     "user": {
      "displayName": "山口哲弘",
      "userId": "06897743095690117498"
     },
     "user_tz": -540
    },
    "id": "Azq4tGofcijx",
    "outputId": "4aef5d9e-42a7-463f-8224-f32f465b0445"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b8ddOSTTROac"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import torchvision.transforms as tvtransforms\n",
    "\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as albu\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SBZiaHOyROag"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# gpu_id=[i for i in range(0, 4)]\n",
    "# device = torch.device(f\"cuda:{gpu_id}\")\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2, 3'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1660718496466,
     "user": {
      "displayName": "山口哲弘",
      "userId": "06897743095690117498"
     },
     "user_tz": -540
    },
    "id": "eitsgfLEr9D8",
    "outputId": "ef25e632-f8c9-4bd7-e449-52ca1d3e63e1"
   },
   "outputs": [],
   "source": [
    "train_dir = './train.txt'\n",
    "val_dir = './val.txt'\n",
    "test_dir = './test.txt'\n",
    "\n",
    "f = open(train_dir, newline='')\n",
    "train = f.readlines()\n",
    "train = [i.rstrip('\\n') for i in train]\n",
    "\n",
    "f = open(val_dir, newline='')\n",
    "val = f.readlines()\n",
    "val = [i.rstrip('\\n') for i in val]\n",
    "\n",
    "f = open(test_dir, newline='')\n",
    "test = f.readlines()\n",
    "test = [i.rstrip('\\n') for i in test]\n",
    "\n",
    "#Train用、Val用、Test用の画像数の確認\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgOEYM0NcaXh"
   },
   "outputs": [],
   "source": [
    "# x_train = os.path.join(DATA_DIR, 'train')\n",
    "# y_train = os.path.join(DATA_DIR, 'train_anno')\n",
    "\n",
    "# x_valid = os.path.join(DATA_DIR, 'val')\n",
    "# y_valid = os.path.join(DATA_DIR, 'val_anno')\n",
    "\n",
    "# x_test = os.path.join(DATA_DIR, 'test')\n",
    "# y_test = os.path.join(DATA_DIR, 'test_anno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train\n",
    "y_train = [s.replace('_img', '').replace('image', 'mask') for s in train]\n",
    "\n",
    "x_valid = val\n",
    "y_valid = [s.replace('_img', '').replace('image', 'mask') for s in val]\n",
    "\n",
    "x_test = test\n",
    "y_test = [s.replace('_img', '').replace('image', 'mask') for s in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E6Q0sk4ZCzaB"
   },
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHspHCDsROaj"
   },
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    mean=[0.485, 0.456, 0.406]\n",
    "    std=[0.229, 0.224, 0.225]\n",
    "    \n",
    "    CLASSES = ['__background__', 'pavement', 'braille_blocks', 'pedestrian', 'unlabelled']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "            \n",
    "    ):\n",
    "\n",
    "    \n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "        # self.mapping = {(0, 0, 0): 0,  # 0 = background\n",
    "        #                 (55, 125, 34): 1,  # 1 = class 1\n",
    "        #                 (0, 9, 123): 2,  # 2 = class 2\n",
    "        #                 (128, 127, 38): 3, # 3 = class 3\n",
    "        #                 (117, 20, 12): 4}  # 4 = class 4\n",
    "\n",
    "        self.mapping = [\n",
    "                    [0, 0, 0],\n",
    "                    [0, 128, 0], # 歩道\n",
    "                    [0, 0, 128], # 点字ブロック\n",
    "                    [128, 128, 0], #人\n",
    "                    [128, 0, 0] # unlabel\n",
    "                ]\n",
    "\n",
    "    def mask_to_class_rgb(self, mask):\n",
    "        print('----mask->rgb----')\n",
    "        mask = torch.from_numpy(np.array(mask))\n",
    "        mask = torch.squeeze(mask)  # remove 1\n",
    "\n",
    "        # check the present values in the mask, 0 and 255 in my case\n",
    "        print('unique values rgb    ', torch.unique(mask)) \n",
    "        # -> unique values rgb     tensor([  0, 255], dtype=torch.uint8)\n",
    "\n",
    "        class_mask = mask\n",
    "        class_mask = class_mask.permute(2, 0, 1).contiguous()\n",
    "        h, w = class_mask.shape[1], class_mask.shape[2]\n",
    "        mask_out = torch.empty(h, w, dtype=torch.long)\n",
    "\n",
    "        for k in self.mapping:\n",
    "            idx = (class_mask == torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))         \n",
    "            validx = (idx.sum(0) == 4)          \n",
    "            mask_out[validx] = torch.tensor(self.mapping[k], dtype=torch.long)\n",
    "\n",
    "        # check the present values after mapping, in my case 0, 1, 2, 3\n",
    "        print('unique values mapped ', torch.unique(mask_out))\n",
    "        # -> unique values mapped  tensor([0, 1, 2, 3])\n",
    "       \n",
    "        return mask_out\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        # ファイル名がマスク画像と元画像で違うため整合性を取る処理\n",
    "        self.masks_fps[i] = self.masks_fps[i].replace(\"_img\", \"\")\n",
    "        # mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        mask = cv2.imread(self.masks_fps[i])\n",
    "\n",
    "        image_pad = np.zeros((image.shape[0]+8, image.shape[1], image.shape[2]), dtype=np.uint8)\n",
    "        image_pad[0:-8] = image\n",
    "        mask_pad = np.zeros((mask.shape[0]+8, mask.shape[1], mask.shape[2]), dtype=np.uint8)\n",
    "        mask_pad[0:-8] = mask\n",
    "\n",
    "        image = image_pad\n",
    "        mask = mask_pad\n",
    "        # print(mask)\n",
    "\n",
    "        image_mask = np.zeros(mask.shape, dtype=np.int8)\n",
    "\n",
    "        for i, c in enumerate(self.mapping):\n",
    "          image_mask[np.where((mask == c).all(axis=2))] = (i, 0, 0)\n",
    "        \n",
    "        mask = image_mask[:, :, 0]\n",
    "      \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
    "        image = t(image)\n",
    "        mask = torch.from_numpy(mask).long()     \n",
    "            \n",
    "        return image, mask\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "make_dir_list = ['train', 'train_anno', 'val', 'val_anno', 'test', 'test_anno']\n",
    "copy_img_list = [x_train, y_train, x_valid, y_valid, x_test, y_test]\n",
    "\n",
    "for i in make_dir_list:\n",
    "    os.makedirs(f'./{i}', exist_ok=True)   \n",
    "    for j in copy_img_list:\n",
    "        for k in j:\n",
    "            shutil.copy(f'{k}', f'./{i}/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join('./', 'train')\n",
    "y_train_dir = os.path.join('./', 'train_anno')\n",
    "\n",
    "x_valid_dir = os.path.join('./', 'val')\n",
    "y_valid_dir = os.path.join('./', 'val_anno')\n",
    "\n",
    "x_test_dir = os.path.join('./', 'test')\n",
    "y_test_dir = os.path.join('./', 'test_anno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xqlCdDw6ROal"
   },
   "outputs": [],
   "source": [
    "#Trainデータの確認\n",
    "\n",
    "dataset = Dataset(x_train_dir, y_train_dir, classes=['pavement', 'braille_blocks', 'pedestrian', 'unlabelled'])\n",
    "\n",
    "image, mask = dataset[0] # get some sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sGo1kTh6ROal"
   },
   "outputs": [],
   "source": [
    "#画像およびマスクの表示用関数\n",
    "\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "n01aeQTHROam",
    "outputId": "c4972303-5520-4d92-cf60-9332636f7f66"
   },
   "outputs": [],
   "source": [
    "# Datasetの画像とマスクの確認。\n",
    "\n",
    "dataset = Dataset(x_train_dir, y_train_dir, classes=['pavement', 'braille_blocks', 'pedestrian', 'unlabelled'])\n",
    "\n",
    "image, mask = dataset[0] # get some sample\n",
    "visualize(\n",
    "    image=image.permute(1, 2, 0), #To tensorでchannel, h, wが画像表示用と異なるため、配列変換\n",
    "    mask=mask\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kRINVWVsROan"
   },
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "\n",
    "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "\n",
    "        albu.IAAAdditiveGaussianNoise(p=0.2),\n",
    "        albu.IAAPerspective(p=0.5),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.IAASharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.PadIfNeeded(384, 480),\n",
    "        \n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def test_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    #test_transform = [\n",
    "    #   albu.PadIfNeeded(384, 480)\n",
    "    #]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0tFVmPj1ROan",
    "outputId": "3b092096-68c3-4868-f8a6-4bc2c6fcb16a"
   },
   "outputs": [],
   "source": [
    "# Augumentation処理後の画像の確認\n",
    "\n",
    "dataset = Dataset(x_train_dir, y_train_dir, classes=['pavement', 'braille_blocks', 'pedestrian', 'unlabelled'], augmentation=get_training_augmentation())\n",
    "\n",
    "image, mask = dataset[12] # get some sample\n",
    "visualize(\n",
    "    image=image.permute(1, 2, 0), \n",
    "    mask=mask\n",
    ")\n",
    "print(len(dataset))\n",
    "print(len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3WLeCad3ROao",
    "outputId": "3a3aba97-d275-4fb8-fedf-1186180fa9c0"
   },
   "outputs": [],
   "source": [
    "#画像サイズの確認\n",
    "\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90meXD5KROao"
   },
   "source": [
    "## Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-5u2b2r_ROaq",
    "outputId": "91293548-f5d1-4ce4-8fc0-1c276a9d1e41"
   },
   "outputs": [],
   "source": [
    "#画像のクラスを設定\n",
    "\n",
    "CLASSES = ['__background__','pavement', 'braille_blocks', 'pedestrian', 'unlabelled']\n",
    "len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "referenced_widgets": [
      "93485188c7b547d78c2344d015350a07"
     ]
    },
    "id": "JQdO4dUtROaq",
    "outputId": "8d8829b1-d414-4fb4-f3ee-e165de17e727"
   },
   "outputs": [],
   "source": [
    "#Semantic Segmentationのモデルを設定\n",
    "\n",
    "model = smp.Unet('efficientnet-b4', encoder_weights='imagenet', classes=len(CLASSES), activation=None)\n",
    "model = torch.nn.DataParallel(model, device_ids=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "W1fkvObjROaq",
    "outputId": "76d98e61-9324-4c29-9ce6-0b2eeb4f0c9b"
   },
   "outputs": [],
   "source": [
    "#Train, Validationのデータセットを作成\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    x_train_dir, \n",
    "    y_train_dir, \n",
    "    augmentation=get_training_augmentation(), \n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    x_valid_dir, \n",
    "    y_valid_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JbARgYRs0ghB",
    "outputId": "5b4ad087-5746-4879-8e03-bcd26029e00b"
   },
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wa0Va04LROar",
    "outputId": "42377764-ac01-4c1a-f977-c5c7e28ae5c2"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xluvNVoROar"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GQRzte-Hd2it"
   },
   "outputs": [],
   "source": [
    "# # Create dummy target image\n",
    "# nb_classes = 5 - 1 # 18 classes + background\n",
    "# idx = np.linspace(0., 1., nb_classes)\n",
    "# cmap = plt.cm.get_cmap('viridis')\n",
    "# rgb = cmap(idx, bytes=True)[:, :3]  # Remove alpha value\n",
    "\n",
    "# h, w = 190, 100\n",
    "# rgb = rgb.repeat(1000, 0)\n",
    "# target = np.zeros((h*w, 3), dtype=np.uint8)\n",
    "# target[:rgb.shape[0]] = rgb\n",
    "# target = target.reshape(h, w, 3)\n",
    "\n",
    "# plt.imshow(target) # Each class in 10 rows\n",
    "\n",
    "# # Create mapping\n",
    "# # Get color codes for dataset (maybe you would have to use more than a single\n",
    "# # image, if it doesn't contain all classes)\n",
    "# target = torch.from_numpy(target)\n",
    "# colors = torch.unique(target.view(-1, target.size(2)), dim=0).numpy()\n",
    "# target = target.permute(2, 0, 1).contiguous()\n",
    "\n",
    "# mapping = {tuple(c): t for c, t in zip(colors.tolist(), range(len(colors)))}\n",
    "\n",
    "# mask = torch.empty(h, w, dtype=torch.long)\n",
    "# for k in mapping:\n",
    "#     # Get all indices for current class\n",
    "#     idx = (target==torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))\n",
    "#     validx = (idx.sum(0) == 3)  # Check that all channels match\n",
    "#     mask[validx] = torch.tensor(mapping[k], dtype=torch.long)\n",
    "# print(mask, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Nmh-bmkUROar"
   },
   "outputs": [],
   "source": [
    "def pixel_accuracy(output, mask):\n",
    "    with torch.no_grad():\n",
    "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
    "        correct = torch.eq(output, mask).int()\n",
    "        accuracy = float(correct.sum()) / float(correct.numel())\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XYlZrAbWROas"
   },
   "outputs": [],
   "source": [
    "def mIoU(pred_mask, mask, smooth=1e-10, n_classes=len(CLASSES)):\n",
    "    with torch.no_grad():\n",
    "        pred_mask = F.softmax(pred_mask, dim=1)\n",
    "        pred_mask = torch.argmax(pred_mask, dim=1)\n",
    "        pred_mask = pred_mask.contiguous().view(-1)\n",
    "        mask = mask.contiguous().view(-1)\n",
    "\n",
    "        iou_per_class = []\n",
    "        for clas in range(0, n_classes): #loop per pixel class\n",
    "            true_class = pred_mask == clas\n",
    "            true_label = mask == clas\n",
    "\n",
    "            if true_label.long().sum().item() == 0: #no exist label in this loop\n",
    "                iou_per_class.append(np.nan)\n",
    "            else:\n",
    "                intersect = torch.logical_and(true_class, true_label).sum().float().item()\n",
    "                union = torch.logical_or(true_class, true_label).sum().float().item()\n",
    "\n",
    "                iou = (intersect + smooth) / (union +smooth)\n",
    "                iou_per_class.append(iou)\n",
    "        return np.nanmean(iou_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "aUB20OgfROas"
   },
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit(epochs, model, train_loader, val_loader, criterion, optimizer, scheduler, patch=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_iou = []; val_acc = []\n",
    "    train_iou = []; train_acc = []\n",
    "    lrs = []\n",
    "    min_loss = np.inf\n",
    "    decrease = 1 ; not_improve=0\n",
    "\n",
    "    model.to(device)\n",
    "    fit_time = time.time()\n",
    "    for e in range(epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        accuracy = 0\n",
    "        #training loop\n",
    "        model.train()\n",
    "        for i, data in enumerate(tqdm(train_loader)):\n",
    "            #training phase\n",
    "            image_tiles, mask_tiles = data\n",
    "            if patch:\n",
    "                bs, n_tiles, c, h, w = image_tiles.size()\n",
    "\n",
    "                image_tiles = image_tiles.view(-1,c, h, w)\n",
    "                mask_tiles = mask_tiles.view(-1, h, w)\n",
    "            \n",
    "            image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
    "            #forward\n",
    "            output = model(image)\n",
    "            # print(output.shape)\n",
    "            # print(mask.shape)\n",
    "            # print(mask.min(), mask.max())\n",
    "            loss = criterion(output, mask)\n",
    "            # output_probs = F.sigmoid(output)\n",
    "            # output_flat = output_probs.view(output_probs.size(0),-1)\n",
    "            # mask_flat = mask.view(mask.size(0),-1)\n",
    "            # print(output_flat.shape)\n",
    "            # print(mask_flat.shape)\n",
    "            # print(output_flat.size(0))\n",
    "            # print(mask_flat.size(0))\n",
    "            # loss = criterion(output_flat, mask_flat)\n",
    "            #evaluation metrics\n",
    "            iou_score += mIoU(output, mask)\n",
    "            accuracy += pixel_accuracy(output, mask)\n",
    "            #backward\n",
    "            loss.backward()\n",
    "            optimizer.step() #update weight          \n",
    "            optimizer.zero_grad() #reset gradient\n",
    "            \n",
    "            #step the learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            scheduler.step() \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        else:\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_accuracy = 0\n",
    "            val_iou_score = 0\n",
    "            #validation loop\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(tqdm(val_loader)):\n",
    "                    #reshape to 9 patches from single image, delete batch size\n",
    "                    image_tiles, mask_tiles = data\n",
    "\n",
    "                    if patch:\n",
    "                        bs, n_tiles, c, h, w = image_tiles.size()\n",
    "\n",
    "                        image_tiles = image_tiles.view(-1,c, h, w)\n",
    "                        mask_tiles = mask_tiles.view(-1, h, w)\n",
    "                    \n",
    "                    image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
    "                    output = model(image)\n",
    "                    #evaluation metrics\n",
    "                    val_iou_score +=  mIoU(output, mask)\n",
    "                    test_accuracy += pixel_accuracy(output, mask)\n",
    "                    #loss\n",
    "                    loss = criterion(output, mask)                                  \n",
    "                    test_loss += loss.item()\n",
    "            \n",
    "            #calculatio mean for each batch\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(val_loader))\n",
    "\n",
    "\n",
    "            if min_loss > (test_loss/len(val_loader)):\n",
    "                print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (test_loss/len(val_loader))))\n",
    "                min_loss = (test_loss/len(val_loader))\n",
    "                decrease += 1\n",
    "                if decrease % 5 == 0:\n",
    "                    print('saving model...')\n",
    "                    #torch.save(model, 'Unet-_mIoU-{:.3f}.pt'.format(val_iou_score/len(val_loader))) #Train途中もモデルを保存するときは実行する\n",
    "                    \n",
    "\n",
    "            if (test_loss/len(val_loader)) > min_loss:\n",
    "                not_improve += 1\n",
    "                min_loss = (test_loss/len(val_loader))\n",
    "                print(f'Loss Not Decrease for {not_improve} time')\n",
    "                if not_improve == 20:\n",
    "                    print('Loss not decrease for 20 times, Stop Training')\n",
    "                    break\n",
    "            \n",
    "            #iou\n",
    "            val_iou.append(val_iou_score/len(val_loader))\n",
    "            train_iou.append(iou_score/len(train_loader))\n",
    "            train_acc.append(accuracy/len(train_loader))\n",
    "            val_acc.append(test_accuracy/ len(val_loader))\n",
    "            print(\"Epoch:{}/{}..\".format(e+1, epochs),\n",
    "                  \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n",
    "                  \"Val Loss: {:.3f}..\".format(test_loss/len(val_loader)),\n",
    "                  \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n",
    "                  \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n",
    "                  \"Train Acc:{:.3f}..\".format(accuracy/len(train_loader)),\n",
    "                  \"Val Acc:{:.3f}..\".format(test_accuracy/len(val_loader)),\n",
    "                  \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "        \n",
    "    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n",
    "               'train_miou' :train_iou, 'val_miou':val_iou,\n",
    "               'train_acc' :train_acc, 'val_acc':val_acc,\n",
    "               'lrs': lrs}\n",
    "    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMFf-uPpROat"
   },
   "outputs": [],
   "source": [
    "max_lr = 1e-3\n",
    "epoch = 2\n",
    "weight_decay = 1e-4\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epoch,\n",
    "                                            steps_per_epoch=len(train_loader))\n",
    "\n",
    "history = fit(epoch, model, train_loader, valid_loader, criterion, optimizer, sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "qqUR2AQ5ROat"
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'Unet-efficientb4_aiseminar.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "lpiL1BsyROat"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history['val_loss'], label='val', marker='o')\n",
    "    plt.plot( history['train_loss'], label='train', marker='o')\n",
    "    plt.title('Loss per epoch'); plt.ylabel('loss');\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_score(history):\n",
    "    plt.plot(history['train_miou'], label='train_mIoU', marker='*')\n",
    "    plt.plot(history['val_miou'], label='val_mIoU',  marker='*')\n",
    "    plt.title('Score per epoch'); plt.ylabel('mean IoU')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_acc(history):\n",
    "    plt.plot(history['train_acc'], label='train_accuracy', marker='*')\n",
    "    plt.plot(history['val_acc'], label='val_accuracy',  marker='*')\n",
    "    plt.title('Accuracy per epoch'); plt.ylabel('Accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(), plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3aaBDCIRROau",
    "outputId": "1d973214-b933-45e8-effc-30eaa6fc63bb"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plot_score(history)\n",
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VbGndFkROau"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-ZpN1jXROau"
   },
   "source": [
    "## Test best saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XMkbjBB_ROau",
    "outputId": "8afca0d1-d837-4e12-d803-620c09f30f1f"
   },
   "outputs": [],
   "source": [
    "# load best saved checkpoint\n",
    "model = torch.load('Unet-efficientb4_qiita.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "J5PwU5meROau"
   },
   "outputs": [],
   "source": [
    "class testDataset(BaseDataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['__background__','pavement', 'braille_blocks', 'pedestrian', 'unlabelled']\n",
    "\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            classes=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(self.masks_fps[i], 0)\n",
    "        \n",
    "        \n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        #t = T.Compose([T.ToTensor()])\n",
    "        #image = t(image)\n",
    "        mask = torch.from_numpy(mask).long()\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VRZ0vdRxROav"
   },
   "outputs": [],
   "source": [
    "# create test dataset\n",
    "test_dataset = testDataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    #preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NBKZ98BHROav"
   },
   "outputs": [],
   "source": [
    "def predict_image_mask_miou(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        \n",
    "        output = model(image)\n",
    "        score = mIoU(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OpcivJqhROav"
   },
   "outputs": [],
   "source": [
    "def predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    model.eval()\n",
    "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    image = t(image)\n",
    "    model.to(device); image=image.to(device)\n",
    "    mask = mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        image = image.unsqueeze(0)\n",
    "        mask = mask.unsqueeze(0)\n",
    "        \n",
    "        output = model(image)\n",
    "        acc = pixel_accuracy(output, mask)\n",
    "        masked = torch.argmax(output, dim=1)\n",
    "        masked = masked.cpu().squeeze(0)\n",
    "    return masked, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Zi8TA0SSROaw"
   },
   "outputs": [],
   "source": [
    "image, mask = test_dataset[3]\n",
    "pred_mask, score = predict_image_mask_miou(model, image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "p2eXCD-oROaw"
   },
   "outputs": [],
   "source": [
    "image, mask = test_dataset[3]\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "A3ub0p5MROaw"
   },
   "outputs": [],
   "source": [
    "def miou_score(model, test_set):\n",
    "    score_iou = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, score = predict_image_mask_miou(model, img, mask)\n",
    "        score_iou.append(score)\n",
    "    return score_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "64dBAEwcROaw"
   },
   "outputs": [],
   "source": [
    "mob_miou = miou_score(model, test_dataset)\n",
    "print('Test Set mIoU', np.mean(mob_miou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-8gxJ1A0ROaw"
   },
   "outputs": [],
   "source": [
    "def pixel_acc(model, test_set):\n",
    "    accuracy = []\n",
    "    for i in tqdm(range(len(test_set))):\n",
    "        img, mask = test_set[i]\n",
    "        pred_mask, acc = predict_image_mask_pixel(model, img, mask)\n",
    "        accuracy.append(acc)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b3lMKv7ROax"
   },
   "outputs": [],
   "source": [
    "mob_acc = pixel_acc(model, test_dataset)\n",
    "print('Test Set Pixel Accuracy', np.mean(mob_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awXUYjHRROax"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('Picture');\n",
    "\n",
    "ax2.imshow(mask)\n",
    "ax2.set_title('Ground truth')\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax3.imshow(pred_mask)\n",
    "ax3.set_title('UNet-MobileNet | mIoU {:.3f}'.format(score))\n",
    "ax3.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z05OZIDfROax"
   },
   "outputs": [],
   "source": [
    "image2, mask2 = test_dataset[15]\n",
    "pred_mask2, score2 = predict_image_mask_miou(model, image2, mask2)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\n",
    "ax1.imshow(image2)\n",
    "ax1.set_title('Picture');\n",
    "\n",
    "ax2.imshow(mask2)\n",
    "ax2.set_title('Ground truth')\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax3.imshow(pred_mask2)\n",
    "ax3.set_title('UNet-MobileNet | mIoU {:.3f}'.format(score2))\n",
    "ax3.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXm6BryfROax"
   },
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7K8bSqsROax"
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    n = np.random.choice(len(test_dataset))\n",
    "    \n",
    "    image2, mask2 = test_dataset[n]\n",
    "    \n",
    "    pred_mask2, score2 = predict_image_mask_miou(model, image2, mask2)\n",
    "    \n",
    "    print('UNet-EfficientNet-B4 | mIoU {:.3f}'.format(score2))\n",
    "    \n",
    "    visualize(\n",
    "        image=image2, \n",
    "        ground_truth=mask2,\n",
    "        predict_mask = pred_mask2,\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1VbGndFkROau"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
